{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9a00a25-05bc-41b8-80c3-e92ffe7d93e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def auto_cralwer(job_key,page_key):\n",
    "    \n",
    "    import requests\n",
    "    from bs4 import BeautifulSoup\n",
    "    import csv\n",
    "    import random, time\n",
    "    from fake_useragent import UserAgent\n",
    "    \n",
    "    delay_choices = [1,2,3,4,5]#延遲秒數\n",
    "    delay = random.choices(delay_choices)#隨機選取秒數\n",
    "\n",
    "    ua = UserAgent()\n",
    "    # proxy = get_random_proxy()\n",
    "    # proxies1 = 'https://' + proxy[0]\n",
    "    # proxies2 = 'http://' + proxy[0]\n",
    "    # proxies3 = {'http': proxies2, 'https':proxies1}\n",
    "\n",
    "    url_a = 'https://www.104.com.tw/jobs/search/?ro=0&keyword='+ str(job_key) +'&expansionType=area,spec,com,job,wf,wktm&order=12&asc=0&page='\n",
    "    url_b = '&mode=s&jobsource=2018indexpoc&langFlag=0&langStatus=0&recommendJob=1&hotJob=1'\n",
    "\n",
    "    all_job_datas = []\n",
    "\n",
    "    for page in range(1,page_key):\n",
    "        url = url_a+ str(page)+url_b\n",
    "        print(url)\n",
    "        headers = {'Referer': 'https://www.104.com.tw/job/6vrlf',\n",
    "                  'User-Agent': ua.random}\n",
    "        response = requests.get(url, headers=headers)\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        jobs = soup.find_all('article',class_='js-job-item')\n",
    "\n",
    "        for job in jobs:\n",
    "            job_name = job.find('a', class_='js-job-link').text\n",
    "            job_company = job.get('data-cust-name')\n",
    "            job_loc = job.find('ul', class_='job-list-intro').find('li').text\n",
    "            job_pay = job.find('span',class_='b-tag--default').text\n",
    "            job_content_link = job.find('a', class_='js-job-link').get('href')\n",
    "            all_job_link = ('https:'+job_content_link)\n",
    "\n",
    "            print('抓職缺大數據中...')\n",
    "\n",
    "            header = {'Referer': 'https://www.104.com.tw/jobs/search/'}\n",
    "            time.sleep(1)\n",
    "            response_link = requests.get(all_job_link, headers = header)\n",
    "            soup_link = BeautifulSoup(response_link.text, 'html.parser')\n",
    "            job_content = soup_link.find('p', class_='job-description__content').text #工作內容\n",
    "            job_require_all = soup_link.find_all('div', class_= 'dialog container-fluid bg-white rounded job-requirement mb-4 pt-6 pb-6')\n",
    "            job_goodat_all = soup_link.find_all('a', class_='tools')\n",
    "            job_other_all = soup_link.find_all('p', class_='m-0 r3 w-100')\n",
    "\n",
    "            job_goodat1 = []\n",
    "            list.clear(job_goodat1)\n",
    "\n",
    "            # for link in all_job_link:\n",
    "            #     url = link\n",
    "            #     header = {'Referer': 'https://www.104.com.tw/jobs/search/'}\n",
    "            #     response_link = requests.get(url, headers = header)\n",
    "            #     time.sleep(1)\n",
    "            #     soup_link = BeautifulSoup(response_link.text, 'html.parser')\n",
    "            #     job_content = soup_link.find('p', class_='job-description__content').text #工作內容\n",
    "            #     job_require_all = soup_link.find_all('div', class_= 'dialog container-fluid bg-white rounded job-requirement mb-4 pt-6 pb-6')\n",
    "            #     job_goodat_all = soup_link.find_all('a', class_='tools')\n",
    "            #     job_other_all = soup_link.find_all('p', class_='m-0 r3 w-100')\n",
    "            #     print('抓detail中...')\n",
    "\n",
    "            for i in job_require_all:\n",
    "                job_exp = i.p.text.strip()#工作經歷\n",
    "                job_dreqire = i.find_all('p')[1].text.strip()#學歷要求\n",
    "                job_mreqire = i.find_all('p')[2].text.strip()#科系要求\n",
    "            for i in job_goodat_all:\n",
    "                job_goodat = i.text #擅長工具\n",
    "                job_goodat1.append(job_goodat)\n",
    "            for i in job_other_all:\n",
    "                job_other = i.text # 其他條件\n",
    "\n",
    "            print('寫入細節')    \n",
    "            job_data = {'職缺':job_name, \n",
    "                        '公司名稱':job_company, \n",
    "                        '地址':job_loc, \n",
    "                        '薪資':job_pay, \n",
    "                        '連結':job_content_link, \n",
    "                        '工作內容':job_content, \n",
    "                        '工作經驗':job_dreqire, \n",
    "                        '科系要求':job_mreqire, \n",
    "                        '擅長工具':job_goodat1, \n",
    "                        '其他條件':job_other}\n",
    "\n",
    "            all_job_datas.append(job_data)\n",
    "            print('資料寫入完成')\n",
    "            time.sleep(delay[0])       \n",
    "        time.sleep(delay[0])\n",
    "\n",
    "def clrawer_save(name):\n",
    "    import csv\n",
    "    fn = name + '.csv'\n",
    "    columns_name = ['職缺','公司名稱','地址', '薪資','連結','工作內容','工作經驗','科系要求',\n",
    "                   '擅長工具','其他條件']\n",
    "    with open(fn, 'w', newline='', encoding='utf-8-sig', errors='backslashreplace') as f:\n",
    "        dictWriter = csv.DictWriter(f, fieldnames = columns_name)\n",
    "        dictWriter.writeheader()\n",
    "        for data in all_job_datas:\n",
    "            dictWriter.writerow(data)\n",
    "    return name"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
